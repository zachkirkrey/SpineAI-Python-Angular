default:
  image: "ubuntu:18.04"

variables:
  # Attempt to reduce Docker disk space usage.
  DOCKER_DRIVER: overlay2
  # Required to set python character encoding for pipenv.
  LC_ALL: C.UTF-8
  LANG: C.UTF-8

stages:
  - test
  - docker-build
  - docker-push
  - verify

test:unittest:api:
  stage: test
  image: node:12.16-buster-slim
  variables:
    CI_PRE_CLONE_SCRIPT: 'git config --global lfs.fetchexclude "backend/bin/models/**/*.hdf5"'
    GIT_STRATEGY: fetch
    NPM_CACHE: "$CI_PROJECT_DIR/backend/.cache/npm"
  cache:
    # Cache key based on the job and git branch/tag name.
    key: "$CI_JOB_NAME-$CI_COMMIT_REF_SLUG"
    paths:
      - .apt
      - backend/.cache/npm
  before_script:
    - rm -f /etc/apt/apt.conf.d/docker-clean
    - mkdir -p .apt && mkdir -p /var/cache/apt/archives && mount --bind .apt /var/cache/apt/archives/
    - apt-get -qq -y update && apt-get -qq -y install
      build-essential
      python3
      python3-dev > /dev/null
  script:
    - cd frontend/restify
    - npm ci --cache $NPM_CACHE --prefer-offline
    - npm test

test:unittest:backend:
  image: "tensorflow/tensorflow:2.2.0"
  stage: test
  tags:
    - theseusai
  variables:
    # Pipenv configuration to support caching between pipeline runs.
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/backend/.cache/pip"
  cache:
    # Cache key based on the job and git branch/tag name.
    key: "$CI_JOB_NAME-$CI_COMMIT_REF_SLUG"
    paths:
      - .apt
      - backend/.cache/pip
  before_script:
    - export PATH=$PATH:/root/.local/bin
    - rm -f /etc/apt/apt.conf.d/docker-clean
    - mkdir -p .apt && mkdir -p /var/cache/apt/archives && mount --bind .apt /var/cache/apt/archives/
    - apt-get -qq -y update && apt-get -qq -y install
      git
      xvfb
      libfontconfig
      fontconfig
      libxrender1
      xfonts-75dpi
      wget
      libpng-dev
      libtiff-dev
      libwebp-dev
      xcftools
      libjpeg-turbo8
      python3
      python3-dev
      python3-pip
      python3-venv > /dev/null
    - dpkg -i backend/deps/wkhtmltox_0.12.5-1.bionic_amd64.deb > /dev/null
  script:
    - cd backend
    - export PYTHONPATH=$(pwd)
    - python3 -m pip install --upgrade pip
    - python3 -m pip install wheel
    - python3 -m pip install --user poetry > /dev/null
    # We reuse tensorflow already installed in the base Docker image.
    - sed -i '/tensorflow\ =/d' ./pyproject.toml
    - rm poetry.lock
    - python3 -m poetry config virtualenvs.create false
    - python3 -m poetry install
    - python3 -m poetry run pip install --find-links=deps torchvision
    - ./run-py-unittests.sh

# Hidden key for building images with docker-compose and and pushing them to
# the Docker Hub.
#
# This pipeline assumes the docker-compose service name is the same as the
# Docker image name (eg. spineai-api -> spineai/spineai-api:latest)
.build:docker: &build-and-push-docker
  stage: docker-build
  tags:
    - theseusai
  image: docker:19.03.9
  interruptible: true
  services:
    - docker:19.03.9-dind
  variables: &build-and-push-docker-variables
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ''
    # If "true", push built images to Docker Hub.
    PUSH_IMAGE: "false"
  before_script:
    - apk add --no-cache docker-compose
    # NOTE(billy): DOCKER_ACCESS_TOKEN is set in the gitlab.com UI.
    - docker login -u spineaibilly -p $DOCKER_ACCESS_TOKEN
  script:
    - cd $SPINEAI_DOCKER_COMPOSE_DIR
    - for image in $SPINEAI_DOCKER_COMPOSE_SERVICES; do
        docker pull "spineai/${image}:latest";
      done
    - if [ -z "$SPINEAI_DOCKER_COMPOSE_FILE" ]; then
        cat docker-compose.yml &&
        docker-compose build $SPINEAI_DOCKER_COMPOSE_SERVICES;
      else
        cat $SPINEAI_DOCKER_COMPOSE_FILE &&
        docker-compose -f $SPINEAI_DOCKER_COMPOSE_FILE build $SPINEAI_DOCKER_COMPOSE_SERVICES;
      fi
    - if [ "$PUSH_IMAGE" == "true" ]; then
        for image in $SPINEAI_DOCKER_COMPOSE_SERVICES; do
          docker push "spineai/${image}:latest";
        done;
      fi

build:docker:backend:
  <<: *build-and-push-docker
  except:
    - master
    - schedules
  interruptible: true
  variables:
    <<: *build-and-push-docker-variables
    SPINEAI_DOCKER_COMPOSE_DIR: 'docker'
    SPINEAI_DOCKER_COMPOSE_FILE: 'dev.docker-compose.yml'
    SPINEAI_DOCKER_COMPOSE_SERVICES: 'spineai-backend'

push:docker:backend:
  <<: *build-and-push-docker
  stage: docker-push
  only:
    - master
  variables:
    <<: *build-and-push-docker-variables
    SPINEAI_DOCKER_COMPOSE_DIR: 'docker'
    SPINEAI_DOCKER_COMPOSE_FILE: 'dev.docker-compose.yml'
    SPINEAI_DOCKER_COMPOSE_SERVICES: 'spineai-backend'
    PUSH_IMAGE: "true"

build:docker:frontend:
  <<: *build-and-push-docker
  except:
    - master
    - schedules
  interruptible: true
  variables:
    <<: *build-and-push-docker-variables
    SPINEAI_DOCKER_COMPOSE_DIR: 'frontend/docker'
    SPINEAI_DOCKER_COMPOSE_SERVICES: 'spineai-api spineai-frontend'
    CI_PRE_CLONE_SCRIPT: 'git config --global lfs.fetchexclude "*"'

push:docker:frontend:
  <<: *build-and-push-docker
  stage: docker-push
  only:
    - master
  variables:
    <<: *build-and-push-docker-variables
    SPINEAI_DOCKER_COMPOSE_DIR: 'frontend/docker'
    SPINEAI_DOCKER_COMPOSE_SERVICES: 'spineai-api spineai-frontend'
    CI_PRE_CLONE_SCRIPT: 'git config --global lfs.fetchexclude "*"'
    PUSH_IMAGE: "true"

.verify:e2e:
  stage: verify
  tags:
    - theseusai
  image: "tensorflow/tensorflow:2.2.0-gpu"
  services:
    # This is a custom-built image that provides the Docker daemon, as well
    # as Nvidia drivers and nvidia-docker for gpu-accelerated docker-in-docker.
    - name: spineai/nvidia-dind:latest
      alias: docker
  variables:
    CI_PRE_CLONE_SCRIPT: 'git config --global lfs.fetchexclude "backend/testdata/*"'
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ''
    POSTGRES_HOST: 'aether.spineai.com'
    POSTGRES_USER: 'postgres'
    POSTGRES_DATABASE: 'gitlab'
    UPLOAD: 'true'
    NUM_STUDIES: '30'
  cache:
    # Cache key based on the job and git branch/tag name.
    key: "$CI_JOB_NAME-$CI_COMMIT_REF_SLUG"
    paths:
      - .apt
  before_script:
    - echo $CI_PIPELINE_SOURCE
    - echo $CI_COMMIT_REF_SLUG
    - if [ "$CI_COMMIT_REF_SLUG" != "master" ]; then
      export UPLOAD="false";
      export NUM_STUDIES="1";
      fi
    # Set up poetry.
    - rm -f /etc/apt/apt.conf.d/docker-clean
    - mkdir -p .apt && mkdir -p /var/cache/apt/archives && mount --bind .apt /var/cache/apt/archives/
    - apt-get -qq -y update && apt-get -qq -y install
      python3
      python3-dev
      python3-pip
      libpq-dev > /dev/null
    - python3 -m pip install --upgrade pip
    - python3 -m pip -q install wheel
    - python3 -m pip -q install poetry
    # Set up docker-ce-cli
    - apt-get -qq -y update && apt-get -qq -y install
      apt-transport-https
      ca-certificates
      curl
      gnupg-agent
      software-properties-common > /dev/null
    - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
    - add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
    - apt-get -qq -y update && apt-get -qq -y install docker-ce-cli
    - docker login -u spineaibilly -p $DOCKER_ACCESS_TOKEN
    # Set up gitlab/qa requirements.
    - python3 -m poetry config virtualenvs.create false
    - (cd gitlab/qa && python3 -m poetry install)
    # Set up mount directories.
    # SHARED_PATH is shared between both the job and dind containers.
    - 'export SHARED_PATH="$(dirname ${CI_PROJECT_DIR})/shared"'
    - echo $SHARED_PATH
    - rm -rf $SHARED_PATH
    - mkdir -p $SHARED_PATH/studies $SHARED_PATH/database $SHARED_PATH/logs $SHARED_PATH/reports
    # Set up spineai-frontend.
    - apt-get -qq -y update && apt-get -qq -y install
      jq
      wget > /dev/null
    - curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    - unzip -qq awscliv2.zip
    - ./aws/install
  script:
    # Fetch studies from PostgreSQL
    - python3 gitlab/qa/get_studies.py
      --db_host $POSTGRES_HOST
      --db_user $POSTGRES_USER
      --db_database $POSTGRES_DATABASE
      --output_dir $SHARED_PATH/studies
      --num_studies $NUM_STUDIES
    - ls $SHARED_PATH/studies
    # Perform classfication.
    - docker run
      --gpus=all
      --mount type=bind,source=$SHARED_PATH/studies,target=/input
      --mount type=bind,source=$SHARED_PATH/database,target=/database
      --mount type=bind,source=$SHARED_PATH/logs,target=/logs
      spineai/spineai-backend:latest
      --input_dir /input
      --log_file /logs/backend.log
      --sqlite_file /database/database.sqlite3
      --nodaemon
    - cat $SHARED_PATH/logs/backend.log
    - ls -l $SHARED_PATH/database
    # Convert results to PDF and HTML.
    - docker run
      -d
      --mount type=bind,source=$SHARED_PATH/database,target=/database
      -p 8080:8080
      spineai/spineai-api:latest
    - sleep 3
    - for name in $(curl -s docker:8080/studies | jq '.[].name'| sed 's/"//g'); do
      mkdir -p "${SHARED_PATH}/reports/${name}";
      wget -q -O "${SHARED_PATH}/reports/${name}/studies.json" "docker:8080/studies?name=${name}";
      wget -q -O "${SHARED_PATH}/reports/${name}/viewer.html" "docker:8080/reports?as=HTML&type=HTML_VIEWER&Studies.name=${name}&sort=-creation_datetime";
      wget -q -O "${SHARED_PATH}/reports/${name}/report.pdf" "docker:8080/reports?as=PDF&type=PDF_SIMPLE&Studies.name=${name}&sort=-creation_datetime";
      wget -q -O "${SHARED_PATH}/reports/${name}/report.json" "docker:8080/reports?as=JSON&type=JSON&Studies.name=${name}&sort=-creation_datetime";
      wget -q -O "${SHARED_PATH}/reports/${name}/report.csv" "docker:8080/reports?as=CSV&type=JSON&Studies.name=${name}&sort=-creation_datetime";
      done;
    - wget -q -O "${SHARED_PATH}/reports/summary.csv" "docker:8080/reports?as=SUMMARYCSV&type=JSON&count=500";
    - ls -l $SHARED_PATH/reports
    # Upload results to Amazon S3.
    - printf -v date '%(%Y-%m-%d)T' -1
    - if [ -z "$CI_COMMIT_REF_SLUG" ]; then CI_COMMIT_REF_SLUG="local"; CI_PIPELINE_SOURCE="local"; fi
    - echo $UPLOAD
    - if [ "$UPLOAD" == "true" ]; then
      aws s3 cp "${SHARED_PATH}/reports" "s3://spineai-gitlab-qa/gitlab_${date}_${CI_PIPELINE_SOURCE}_${CI_COMMIT_REF_SLUG}" --recursive;
      fi
  after_script:
    # Clean up shared directory.
    - rm -rf $SHARED_PATH
